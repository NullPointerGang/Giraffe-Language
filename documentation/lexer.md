# Документация для лексера (токенайзера)

## Обзор

Лексер — это компонент интерпретатора, который выполняет разбиение исходного кода на последовательность токенов. Токены — это атомарные элементы программы, такие как ключевые слова, операторы, идентификаторы и литералы. Лексер находит эти токены с использованием регулярных выражений и классифицирует их по типу.

Он использует регулярные выражения для анализа строк и выявления различных токенов.

## Структуры данных

### `TokenType`

Перечисление, которое описывает типы токенов, которые могут быть распознаны лексером. Включает в себя:

- **COMMENT_MULTILINE**: Многострочные комментарии.
- **COMMENT_SINGLELINE**: Однострочные комментарии.
- **KEYWORD**: Ключевые слова языка.
- **IDENTIFIER**: Идентификаторы (имена переменных, функций и т.д.).
- **INTEGER**: Целые числа.
- **FLOAT**: Числа с плавающей точкой.
- **STRING**: Строки (с двойными кавычками).
- **BOOLEAN**: Логические значения (true, false).
- **ANNOTATION_TYPE**: Аннотации типов (например, int, str, bool).
- **LIST, DICT, SET, TUPLE**: Сложные типы данных (списки, множества, кортежи).
- **NULL**: Тип данных Null.
- **OPTION**: Тип данных Option.
- **ERROR**: Ошибки в исходном коде.
- **FUNCTION**: Функции.
- **OPERATOR**: Операторы (например, +, -, =).
- **SYMBOL**: Символы (например, $, #).
- **BRACKET**: Скобки (круглые, фигурные, квадратные).
- **PUNCTUATION**: Пунктуация (например, ;, :, .).
- **PRINT**: Специальный токен для команды "print!".
- **EOF**: Конец файла.

### `Token`

Структура, которая представляет собой токен. Каждый токен состоит из:

- **token_type**: Тип токена (например, `KEYWORD`, `IDENTIFIER` и т.д.).
- **value**: Строковое значение токена, которое может быть ключевым словом, идентификатором, числом и т.д.

Метод `new` создает новый токен с указанным типом и значением.

### `Lexer`

Лексер, который выполняет разбиение исходного кода на токены с использованием регулярных выражений. В нем содержатся:

- **keywords**: Хеш-таблица ключевых слов, где ключ — строка, а значение — тип токена (`KEYWORD`).
- **regex_patterns**: Список пар, каждая из которых состоит из регулярного выражения и соответствующего типа токена (или `None`, если токен не создается).

Метод `new` инициализирует лексер, добавляет ключевые слова и регулярные выражения для распознавания различных токенов.

Метод `tokenize` выполняет разбор строки исходного кода на токены, проверяя каждый символ с помощью регулярных выражений. Он возвращает результат в виде вектора токенов.

## Регулярные выражения

Лексер использует регулярные выражения для распознавания токенов в исходном коде. Некоторые из них:

- **Комментарии**:
  - Однострочные комментарии: `//[^\n]*`
  - Многострочные комментарии: `/\*[\s\S]*?\*/`
- **Типы данных**:
  - Примитивные типы: `\b(int|str|bool)\b`
  - Сложные типы: `\b(list|dict|set|tuple)\b`
- **Ключевые слова**:
  - Множество ключевых слов (например, `const`, `func`, `if`, `while`): `\b(?:const|func|var|if|elif|else|while|for|loop|continue|exit|break|return|is|in|and|or|not|try|handle)\b`
- **Числа**:
  - Целые числа: `\b\d+\b`
  - Числа с плавающей точкой: `\b\d+\.\d+\b`
- **Строки**: `#""([^"\\]|\\.)*""#`
- **Операторы**: `[+\-*/%=<>!&|^]=?|==|!=|&&|\|\|`
- **Скобки**: `[\(\)\{\}\[\]]`
- **Пунктуация**: `[;,:.]`

Каждое регулярное выражение привязывается к соответствующему типу токена или игнорируется (например, пробелы).

## Алгоритм работы

1. Лексер получает строку исходного кода.
2. Проходит по строке символ за символом, пытаясь найти подходящие регулярные выражения.
3. Когда выражение находит совпадение, создается токен, который добавляется в список.
4. Если регулярное выражение не находит совпадений, лексер генерирует ошибку.
5. Лексер продолжает до тех пор, пока не обработает всю строку.
6. После завершения обработки добавляется специальный токен `EOF` для обозначения конца файла.


## Заключение

Лексер является важным элементом системы анализа исходного кода. Он преобразует строку текста в структуру токенов, которую затем может обработать синтаксический анализатор. Этот лексер поддерживает множество типов токенов, включая комментарии, ключевые слова, типы данных, операторы и другие элементы синтаксиса языка.